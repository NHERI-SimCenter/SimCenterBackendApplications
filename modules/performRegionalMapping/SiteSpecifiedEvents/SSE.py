# -*- coding: utf-8 -*-
#
# Copyright (c) 2018 Leland Stanford Junior University
# Copyright (c) 2018 The Regents of the University of California
#
# This file is part of the SimCenter Backend Applications
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#
# 1. Redistributions of source code must retain the above copyright notice,
# this list of conditions and the following disclaimer.
#
# 2. Redistributions in binary form must reproduce the above copyright notice,
# this list of conditions and the following disclaimer in the documentation
# and/or other materials provided with the distribution.
#
# 3. Neither the name of the copyright holder nor the names of its contributors
# may be used to endorse or promote products derived from this software without
# specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.
#
# You should have received a copy of the BSD 3-Clause License along with
# this file. If not, see <http://www.opensource.org/licenses/>.
#
# Contributors:
# Stevan Gavrilovic
# Adam ZsarnÃ³czay
# Tamika Bassman
#

import argparse, json
from pathlib import Path
import numpy as np
import pandas as pd
from scipy.cluster.vq import vq

def create_event(asset_file, event_grid_file):

    # read the event grid data file
    event_grid_path = Path(event_grid_file).resolve()
    event_dir = event_grid_path.parent
    event_grid_file = event_grid_path.name

    grid_df = pd.read_csv(event_dir / event_grid_file, header=0)
    
    # store the locations of the grid points in X
    lat_E = grid_df['Latitude']
    lon_E = grid_df['Longitude']
    X = np.array([[lo, la] for lo, la in zip(lon_E, lat_E)])
    
    # load the asset data file
    with open(asset_file, 'r') as f:
        asset_dict = json.load(f)


    # prepare a dataframe that holds asset filenames and locations
    AIM_df = pd.DataFrame(columns=['Latitude', 'Longitude', 'file'], index=np.arange(len(asset_dict)))
    
    for i, asset in enumerate(asset_dict):
        with open(asset['file'], 'r') as f:
            asset_data = json.load(f)

        asset_loc = asset_data['GeneralInformation']['location']
        AIM_df.iloc[i]['Longitude'] = asset_loc['longitude']
        AIM_df.iloc[i]['Latitude'] = asset_loc['latitude']
        AIM_df.iloc[i]['file'] = asset['file']
        
        
    # store asset locations in Y
    Y = np.array([[lo, la] for lo, la in zip(AIM_df['Longitude'], AIM_df['Latitude'])])
    
#    print("****Y",Y)
    
    #print(Y)
    #print(sub_grid)
    
    # Find the index of the closest point - each index corresponds to the gridpoint index
    closest, distances = vq(Y, X)
    
#    print("****closest",closest)
#    print("****distances",distances)
#
#    print("****num found",len(closest))
#    print("****num Y",np.size(Y, 0))
#    print("****num X",np.size(X, 0))


    # check to ensure we found all of the assets
    if len(closest) != np.size(Y, 0) :
        print("Error, the number of assets needs to be equal to the number of grid points")
        print("The number of assets is "+str(np.size(Y, 0))+" and the number of grid points is " + len(closest))
        return 1
        
        
    # iterate through the assets and store the selected events in the AIM
    for idx, AIM_id in enumerate(AIM_df.index):

        # open the AIM file
        asset_file = AIM_df.iloc[AIM_id]['file']
       
        with open(asset_file, 'r') as f:
            asset_data = json.load(f)
        
        # this is the preferred behavior, the else caluse is left for legacy inputs
        if grid_df.iloc[0]['GP_file'][-3:] == 'csv':

            # We assume that every grid point has the same type and number of
            # event data. That is, you cannot mix ground motion records and
            # intensity measures and you cannot assign 10 records to one point
            # and 15 records to another.

            # Load the first file and identify if this is a grid of IM or GM
            # information. GM grids have GM record filenames defined in the
            # grid point files.
            first_file = pd.read_csv(event_dir / grid_df.iloc[0]['GP_file'],header=0)
            
            if first_file.columns[0]=='TH_file':
                event_type = 'timeHistory'
            else:
                event_type = 'intensityMeasure'
                
            event_count = first_file.shape[0]

            # collect the list of events and scale factors
            event_list = []
            scale_list = []
            
            closestPnt = grid_df.iloc[closest[idx]]

            # if the grid has ground motion records...
            if event_type == 'timeHistory':

                # load the file for the selected grid point
                event_collection_file = closestPnt['GP_file']
                
                event_df = pd.read_csv(event_dir / event_collection_file, header=0)

                # append the GM record name to the event list
                event_list.append(event_df.iloc[idx,0])

                # append the scale factor (or 1.0) to the scale list
                if len(event_df.columns) > 1:
                    scale_list.append(float(event_df.iloc[idx,1]))
                else:
                    scale_list.append(1.0)

            # if the grid has intensity measures
            elif event_type == 'intensityMeasure':

                # save the collection file name and the IM row id
                event_list.append(closestPnt['GP_file']+f'x{0}')

                # IM collections are not scaled
                scale_list.append(1.0)

        # TODO: update the LLNL input data and remove this clause
        else:
            event_list = []
            for e, i in zip(nbr_samples, ind_list):
                event_list += [closestPnt['GP_file'],]*e

            scale_list = np.ones(len(event_list))

        # prepare a dictionary of events
        event_list_json = []
        for e_i, event in enumerate(event_list):
            #event_list_json.append({
            #    #"EventClassification": "Earthquake",
            #    "fileName": f'{event}x{e_i:05d}',
            #    "factor": scale_list[e_i],
            #    #"type": event_type
            #    })
            event_list_json.append([f'{event}x{e_i:05d}', scale_list[e_i]])

        # save the event dictionary to the AIM
        asset_data['Events'] = {
            #"EventClassification": "Earthquake",
            "EventFolderPath": str(event_dir),
            "Events": event_list_json,
            "type": event_type
            #"type": "SimCenterEvents"
        }

        with open(asset_file, 'w') as f:
            json.dump(asset_data, f, indent=2)

if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('--assetFile')
    parser.add_argument('--filenameEVENTgrid')
    args = parser.parse_args()

    create_event(args.assetFile, args.filenameEVENTgrid)
