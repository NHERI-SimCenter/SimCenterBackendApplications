# -*- coding: utf-8 -*-
# JGA
import os
import numpy as np
import pandas as pd
import random
from math import pi, sqrt
import PLoM_library as plom

# import matplotlib.pyplot as plt
import warnings

# export DISPLAY=localhost:0.0
from ctypes import *
import importlib
from pathlib import Path
import sys
from general import *


class PLoM:
    def __init__(
        self,
        model_name='plom',
        data='',
        separator=',',
        col_header=False,
        constraints=None,
        run_tag=False,
        plot_tag=False,
        num_rlz=5,
        tol_pca=1e-6,
        epsilon_kde=25,
        tol_PCA2=1e-5,
        tol=1e-6,
        max_iter=50,
        runDiffMaps=True,
        db_path=None,
    ):
        # basic setups
        self._basic_config(model_name=model_name, db_path=db_path)
        self.plot_tag = plot_tag
        # initialize constraints
        self.constraints = {}
        self.num_constraints = 0
        #
        self.runDiffMaps = runDiffMaps
        # initialize input data
        if self.initialize_data(data, separator, col_header):
            self.logfile.write_msg(
                msg='PLoM: data loading failed.', msg_type='ERROR', msg_level=0
            )
        else:
            """
            # plot data matrix
            if self.plot_tag:
                smp = pd.plotting.scatter_matrix(self.X0, alpha=0.5, diagonal ='kde', figsize=(10,10))
                for ax in smp.ravel():
                    ax.set_xlabel(ax.get_xlabel(), fontsize = 6, rotation = 45)
                    ax.set_ylabel(ax.get_ylabel(), fontsize = 6, rotation = 45)
                plt.savefig(os.path.join(self.vl_path,'ScatterMatrix_X0.png'),dpi=480)
                self.logfile.write_msg(msg='PLoM: {} saved in {}.'.format('ScatterMatrix_X0.png',self.vl_path),msg_type='RUNNING',msg_level=0)
            """
        if not self.constraints:
            if self.add_constraints(constraints_file=constraints):
                self.logfile.write_msg(
                    msg='PLoM: constraints input failed.',
                    msg_type='ERROR',
                    msg_level=0,
                )
        # run
        if run_tag:
            self.logfile.write_msg(
                msg='PLoM: Running all steps to generate new samples.',
                msg_type='RUNNING',
                msg_level=0,
            )
            self.ConfigTasks()
            self.RunAlgorithm(
                n_mc=num_rlz,
                epsilon_pca=tol_pca,
                epsilon_kde=epsilon_kde,
                tol_PCA2=tol_PCA2,
                tol=tol,
                max_iter=max_iter,
                plot_tag=plot_tag,
                runDiffMaps=self.runDiffMaps,
            )
        else:
            self.logfile.write_msg(
                msg='PLoM: using ConfigTasks(task_list = FULL_TASK_LIST) to schedule a run.',
                msg_type='RUNNING',
                msg_level=0,
            )
            self.logfile.write_msg(
                msg='PLoM: using RunAlgorithm(n_mc=n_mc,epsilon_pca=epsilon_pca,epsilon_kde) to run simulations.',
                msg_type='RUNNING',
                msg_level=0,
            )

    def _basic_config(self, model_name=None, db_path=None):
        """
        Basic setups
        - model_name: job name (used for database name)
        """
        if not db_path:
            self.dir_log = os.path.join(
                os.path.dirname(os.path.abspath(__file__)), 'RunDir'
            )
            self.dir_run = os.path.join(
                os.path.dirname(os.path.abspath(__file__)), 'RunDir', model_name
            )
        else:
            self.dir_log = db_path
            self.dir_run = os.path.join(db_path, model_name)
        # initialize logfile
        try:
            os.makedirs(self.dir_run, exist_ok=True)
            self.logfile = Logfile(logfile_dir=self.dir_log)
            self.logfile.write_msg(
                msg='PLoM: Running directory {} initialized.'.format(self.dir_run),
                msg_type='RUNNING',
                msg_level=0,
            )
        except:
            self.logfile.write_msg(
                msg='PLoM: Running directory {} cannot be initialized.'.format(
                    self.dir_run
                ),
                msg_type='ERROR',
                msg_level=0,
            )
        # initialize database server
        self.dbserver = None
        self.dbserver = DBServer(db_dir=self.dir_run, db_name=model_name + '.h5')
        try:
            self.dbserver = DBServer(db_dir=self.dir_run, db_name=model_name + '.h5')
        except:
            self.logfile.write_msg(
                msg='PLoM: database server initialization failed.',
                msg_type='ERROR',
                msg_level=0,
            )
        if self.dbserver:
            self.logfile.write_msg(
                msg='PLoM: database server initialized.',
                msg_type='RUNNING',
                msg_level=0,
            )
        # initialize visualization output path
        self.vl_path = os.path.join(self.dir_run, 'FigOut')
        try:
            os.makedirs(self.vl_path, exist_ok=True)
            self.logfile.write_msg(
                msg='PLoM: visualization folder {} initialized.'.format(
                    self.vl_path
                ),
                msg_type='RUNNING',
                msg_level=0,
            )
        except:
            self.logfile.write_msg(
                msg='PLoM: visualization folder {} not initialized.'.format(
                    self.vl_path
                ),
                msg_type='WARNING',
                msg_level=0,
            )

    def add_constraints(self, constraints_file=None):
        if not constraints_file:
            self.g_c = None
            self.D_x_g_c = None
            self.beta_c = []
            self.beta_c_aux = None
            self.lambda_i = 0
            self.psi = 0
            self.logfile.write_msg(
                msg='PLoM.add_constraints: no user-defined constraint - please use add_constraints(constraints_file=X) to add new constraints if any.',
                msg_type='WARNING',
                msg_level=0,
            )
            return 0

        try:
            # path
            path_constraints = Path(constraints_file).resolve()
            sys.path.insert(0, str(path_constraints.parent) + '/')
            # load the function
            new_constraints = importlib.__import__(
                path_constraints.name[:-3], globals(), locals(), [], 0
            )
        except:
            self.logfile.write_msg(
                msg='PLoM.add_constraints: could not add constraints {}'.format(
                    constraints_file
                ),
                msg_type='ERROR',
                msg_level=0,
            )
            return 1
        self.num_constraints = self.num_constraints + 1
        try:
            self.constraints.update(
                {
                    'Constraint' + str(self.num_constraints): {
                        'filename': constraints_file,
                        'g_c': new_constraints.g_c,
                        'D_x_g_c': new_constraints.D_x_g_c,
                        'beta_c': new_constraints.beta_c(),
                        'beta_c_aux': new_constraints.beta_c_aux,
                    }
                }
            )
            self.g_c = new_constraints.g_c
            self.D_x_g_c = new_constraints.D_x_g_c
            self.beta_c = new_constraints.beta_c()
            self.beta_c_aux = new_constraints.beta_c_aux
            self.logfile.write_msg(
                msg='PLoM.add_constraints: constraints added.',
                msg_type='RUNNING',
                msg_level=0,
            )
            self.dbserver.add_item(
                item=[constraints_file], data_type='ConstraintsFile'
            )
        except:
            self.logfile.write_msg(
                msg='PLoM.add_constraints: at least one attribute (i.e., g_c, D_x_gc, beta_c, or beta_c_aux) missing in {}'.format(
                    constraints_file
                ),
                msg_type='ERROR',
                msg_level=0,
            )
            return 1
        return 0

    def switch_constraints(self, constraint_tag=1):
        """
        Selecting different constraints
        - constraint_tag: the tag of selected constraint
        """

        if constraint_tag > self.num_constraints:
            self.logfile.write_msg(
                msg='PLoM.switch_constraints: sorry the maximum constraint tag is {}'.format(
                    self.num_constraints
                ),
                msg_type='ERROR',
                msg_level=0,
            )
        try:
            self.g_c = self.constraints.get('Constraint' + str(constraint_tag)).get(
                'g_c'
            )
            self.D_x_g_c = self.constraints.get(
                'Constraint' + str(constraint_tag)
            ).get('D_x_g_c')
            self.beta_c = self.constraints.get(
                'Constraint' + str(constraint_tag)
            ).get('beta_c')
            self.beta_c_aux = self.constraints.get(
                'Constraint' + str(constraint_tag)
            ).get('beta_c_aux')
            self.dbserver.add_item(
                item=[
                    self.constraints.get('Constraint' + str(constraint_tag)).get(
                        'filename'
                    )
                ],
                data_type='ConstraintsFile',
            )
        except:
            self.logfile.write_msg(
                msg='PLoM.get_constraints: cannot get constraints',
                msg_type='ERROR',
                msg_level=0,
            )

    def delete_constraints(self):
        """
        Removing all current constraints
        """

        self.g_c = None
        self.D_x_g_c = None
        self.beta_c = []
        self.dbserver.add_item(item=[''], data_type='ConstraintsFile')

    def load_data(self, filename, separator=',', col_header=False):
        # initialize the matrix and data size
        X = []
        N = 0
        n = 0

        # check if the file exist
        import os

        if not os.path.exists(filename):
            self.logfile.write_msg(
                msg='load_data: the input file {} is not found'.format(filename),
                msg_type='ERROR',
                msg_level=0,
            )
            return X, N, n

        # read data
        if os.path.splitext(filename)[-1] in ['.csv', '.dat', '.txt']:
            # txt data
            col = None
            if col_header:
                col = 0
            self.X0 = pd.read_table(filename, delimiter=separator, header=col)
            # remove all-nan column if any
            for cur_col in self.X0.columns:
                if all(np.isnan(self.X0.loc[:, cur_col])):
                    self.X0.drop(columns=cur_col)
            X = self.X0.to_numpy()

        elif os.path.splitext(filename)[-1] in ['.mat', '.json']:
            # json or mat
            if os.path.splitext(filename)[-1] == '.mat':
                import scipy.io as scio

                matdata = scio.loadmat(filename)
                var_names = [
                    x for x in list(matdata.keys()) if not x.startswith('__')
                ]
                if len(var_names) == 1:
                    # single matrix
                    X = matdata[var_names[0]]
                    self.X0 = pd.DataFrame(
                        X, columns=['Var' + str(x) for x in X.shape[1]]
                    )
                else:
                    n = len(var_names)
                    # multiple columns
                    for cur_var in var_names:
                        X.append(matdata[cur_var].tolist())
                    X = np.array(X).T
                    X = X[0, :, :]
                    self.X0 = pd.DataFrame(X, columns=var_names)
            else:
                import json

                with open(filename, 'r', encoding='utf-8') as f:
                    jsondata = json.load(f)
                var_names = list(jsondata.keys())
                # multiple columns
                for cur_var in var_names:
                    X.append(jsondata[cur_var])
                X = np.array(X).T
                self.X0 = pd.DataFrame(X, columns=var_names)

        elif os.path.splitext(filename)[-1] in ['.h5']:
            # this h5 can be either formatted by PLoM or not
            # a separate method to deal with this file
            X = self.load_h5(filename)

        else:
            self.logfile.write_msg(
                msg='PLoM.load_data: the file format is not supported yet.',
                msg_type='ERROR',
                msg_level=0,
            )
            self.logfile.write_msg(
                msg='PLoM.load_data: accepted data formats - csv, dat, txt, mat, json.',
                msg_type='WARNING',
                msg_level=0,
            )

        # Update data sizes
        N, n = X.shape
        self.logfile.write_msg(
            msg='PLoM.load_data: loaded data size = ({}, {}).'.format(N, n),
            msg_type='RUNNING',
            msg_level=0,
        )

        # Return data and data sizes
        return X.T, N, n

    # def check_var_name():

    def get_data(self):
        # return data and data sizes
        return self.X, self.N, self.n

    def _load_h5_plom(self, filename):
        """
        Loading PLoM-formatted h5 database
        """
        try:
            store = pd.HDFStore(filename, 'r')
            for cur_var in store.keys():
                if cur_var in self.dbserver.get_item_adds() and ATTR_MAP[cur_var]:
                    # read in
                    cur_data = store[cur_var]
                    cur_dshape = tuple(
                        [x[0] for x in store['/DS_' + cur_var[1:]].values.tolist()]
                    )
                    if cur_dshape == (1,):
                        item_value = np.array(sum(cur_data.values.tolist(), []))
                        col_headers = list(cur_data.columns)[0]
                    else:
                        item_value = cur_data.values
                        col_headers = list(cur_data.columns)
                    self.dbserver.add_item(
                        item_name=cur_var.replace('/', ''),
                        col_names=col_headers,
                        item=item_value,
                        data_shape=cur_dshape,
                    )
                # constraints
                if cur_var == '/constraints_file':
                    cur_data = store[cur_var]
                    self.dbserver.add_item(
                        item=cur_data.values.tolist()[0], data_type='ConstraintsFile'
                    )
            store.close()

        except:
            self.logfile.write_msg(
                msg='PLoM._load_h5_plom: data in {} not compatible.'.format(
                    filename
                ),
                msg_type='ERROR',
                msg_level=0,
            )

    def _load_h5_data_X(self, filename):
        """
        Loading a h5 data which is expected to contain X data
        """
        try:
            store = pd.HDFStore(filename, 'r')
            # Note a table is expected for the variable
            self.X0 = store.get(store.keys()[0])
            store.close()
            self.dbserver.add_item(
                item_name='X0', col_name=list(self.X0.columns), item=self.X0
            )

            return self.X0.to_numpy()
        except:
            return None

    def _sync_data(self):
        """
        Sync database data to current attributes
        """
        avail_name_list = self.dbserver.get_name_list()
        if not avail_name_list:
            # empty database
            self.logfile.write_msg(
                msg='PLoM._sync_data: database is empty - no data to sync.',
                msg_type='WARNING',
                msg_level=0,
            )
        else:
            for cur_item in avail_name_list:
                if cur_item.startswith('/DS_'):
                    # skipping the data-shape attributes
                    continue
                if type(ATTR_MAP[cur_item]) is str:
                    self.__setattr__(
                        ATTR_MAP[cur_item], self.dbserver.get_item(cur_item[1:])
                    )
                    self.logfile.write_msg(
                        msg='PLoM._sync_data: self.{} synced.'.format(
                            ATTR_MAP[cur_item]
                        ),
                        msg_type='RUNNING',
                        msg_level=0,
                    )
                else:
                    # None type (this is the 'basic' - skipped)
                    self.logfile.write_msg(
                        msg='PLoM._sync_data: data {} skipped.'.format(cur_item),
                        msg_type='RUNNING',
                        msg_level=0,
                    )

    def _sync_constraints(self):
        """
        Sync constraints from dbserver to the attributes
        """
        avail_name_list = self.dbserver.get_name_list()
        if '/constraints_file' not in avail_name_list:
            # empty constraints
            self.logfile.write_msg(
                msg='PLoM._sync_data: no available constraint to sync.',
                msg_type='WARNING',
                msg_level=0,
            )
        else:
            # get constraints file path
            cfile = self.dbserver.get_item(data_type='ConstraintsFile')
            # add the constraints
            self.add_constraints(constraints_file=cfile)

    def load_h5(self, filename):
        """
        Loading h5 database
        """
        try:
            self._load_h5_plom(filename)
            self.logfile.write_msg(
                msg='PLoM.load_h5: h5 file loaded.', msg_type='RUNNING', msg_level=0
            )
            # sync data
            self._sync_data()
            self.logfile.write_msg(
                msg='PLoM.load_h5: data in {} synced.'.format(filename),
                msg_type='RUNNING',
                msg_level=0,
            )
            self._sync_constraints()
            self.logfile.write_msg(
                msg='PLoM.load_h5: constraints in {} synced.'.format(filename),
                msg_type='RUNNING',
                msg_level=0,
            )
            if '/X0' in self.dbserver.get_name_list():
                self.X0 = self.dbserver.get_item('X0', table_like=True)
                return self.X0.to_numpy()
            else:
                self.logfile.write_msg(
                    msg='PLoM.load_h5: the original X0 data not found in the loaded data.',
                    msg_type='ERROR',
                    msg_level=0,
                )
                return None
        except:
            X = self._load_h5_data_X(filename)
            if X is None:
                self.logfile.write_msg(
                    msg='PLoM.load_h5: cannot load {}.'.format(filename),
                    msg_type='ERROR',
                    msg_level=0,
                )
                return None
            else:
                return X

    def add_data(self, filename, separator=',', col_header=False):
        # load new data
        new_X, new_N, new_n = self.load_data(filename, separator, col_header)
        # check data sizes
        if new_n != self.n:
            self.logfile.write_msg(
                msg='PLoM.add_data: incompatible column size when loading {}'.format(
                    filename
                ),
                msg_type='ERROR',
                msg_level=0,
            )
        else:
            # update the X and N
            self.X = np.concatenate((self.X, new_X))
            self.N = self.N + new_N
            self.X0.append(pd.DataFrame(new_X.T, columns=list(self.X0.columns)))

        self.logfile.write_msg(
            msg='PLoM.add_data: current X0 size = ({}, {}).'.format(self.N, self.n),
            msg_type='RUNNING',
            msg_level=0,
        )

    def initialize_data(
        self, filename, separator=',', col_header=False, constraints=''
    ):
        # initialize the data and data sizes
        try:
            self.X, self.N, self.n = self.load_data(filename, separator, col_header)
        except:
            self.logfile.write_msg(
                msg='PLoM.initialize_data: cannot initialize data with {}'.format(
                    filename
                ),
                msg_type='ERROR',
                msg_level=0,
            )
            return 1

        # Save to database
        self.dbserver.add_item(
            item_name='X0',
            col_names=list(self.X0.columns),
            item=self.X.T,
            data_shape=self.X.shape,
        )
        self.dbserver.add_item(item_name='N', item=np.array([self.N]))
        self.dbserver.add_item(item_name='n', item=np.array([self.n]))
        self.logfile.write_msg(
            msg='PLoM.initialize_data: current X0 size = ({}, {}).'.format(
                self.N, self.n
            ),
            msg_type='RUNNING',
            msg_level=0,
        )
        self.logfile.write_msg(
            msg='PLoM.initialize_data: X0 and X0_size saved to database.',
            msg_type='RUNNING',
            msg_level=0,
        )

        return 0

    def _init_indv_tasks(self):
        """
        Initializing tasks
        """
        for cur_task in FULL_TASK_LIST:
            self.__setattr__('task_' + cur_task, Task(task_name=cur_task))

    def ConfigTasks(self, task_list=FULL_TASK_LIST):
        """
        Creating a task list object
        - task_list: a string list of tasks to run
        """
        config_flag = True
        self.cur_task_list = task_list
        # check task orders
        if not all([x in FULL_TASK_LIST for x in self.cur_task_list]):
            self.logfile.write_msg(
                msg='PLoM.config_tasks: task name not recognized.',
                msg_type='ERROR',
                msg_level=0,
            )
            self.logfile.write_msg(
                msg='PLoM.config_tasks: acceptable task names: {}.'.format(
                    ','.join(FULL_TASK_LIST)
                ),
                msg_type='WARNING',
                msg_level=0,
            )
            return False
        map_order = [FULL_TASK_LIST.index(x) for x in self.cur_task_list]
        if map_order != sorted(map_order):
            self.logfile.write_msg(
                msg='PLoM.config_tasks: task order error.',
                msg_type='ERROR',
                msg_level=0,
            )
            self.logfile.write_msg(
                msg='PLoM.config_tasks: please follow this order: {}.'.format(
                    '->'.join(FULL_TASK_LIST)
                ),
                msg_type='WARNING',
                msg_level=0,
            )
            return False
        if (max(map_order) - min(map_order) + 1) != len(map_order):
            # intermediate tasks missing -> since the jobs are in chain, so here the default is to automatically fill in any missing tasks in the middle
            self.cur_task_list = FULL_TASK_LIST[min(map_order) : max(map_order) + 1]
            self.logfile.write_msg(
                msg='PLoM.config_tasks: intermediate task(s) missing and being filled in automatically.',
                msg_type='WARNING',
                msg_level=0,
            )
            self.logfile.write_msg(
                msg='PLoM.config_tasks: the filled task list is: {}.'.format(
                    '->'.join(self.cur_task_list)
                ),
                msg_type='RUNNING',
                msg_level=0,
            )
        # intializing the task list
        self.task_list = TaskList()
        # intializing individual tasks and refreshing status
        self._init_indv_tasks()
        for cur_task in FULL_TASK_LIST:
            self.__getattribute__('task_' + cur_task).full_var_list = TASK_ITEM_MAP[
                cur_task
            ]
            for cur_item in TASK_ITEM_MAP[cur_task]:
                if '/' + cur_item in self.dbserver.get_name_list():
                    self.__getattribute__('task_' + cur_task).avail_var_list.append(
                        cur_item
                    )
                self.__getattribute__('task_' + cur_task).refresh_status()
        # create the task list
        for cur_task in self.cur_task_list:
            self.task_list.add_task(
                new_task=self.__getattribute__('task_' + cur_task)
            )

        self.task_list.refresh_status()
        # need to check the task chain if all dependent tasks completed to go
        # otherwise, the current run could not be completed
        pre_task_list = FULL_TASK_LIST[: FULL_TASK_LIST.index(self.cur_task_list[0])]
        if len(pre_task_list):
            for cur_task in pre_task_list:
                if not self.__getattribute__('task_' + cur_task).refresh_status():
                    config_flag = False
                    self.logfile.write_msg(
                        msg='PLoM.config_tasks: configuration failed with dependent task {} not completed.'.format(
                            cur_task
                        ),
                        msg_type='ERROR',
                        msg_level=0,
                    )

        if config_flag:
            self.logfile.write_msg(
                msg='PLoM.config_tasks: the following tasks is configured to run: {}.'.format(
                    '->'.join(self.cur_task_list)
                ),
                msg_type='RUNNING',
                msg_level=0,
            )

    def RunAlgorithm(
        self,
        n_mc=5,
        epsilon_pca=1e-6,
        epsilon_kde=25,
        tol_PCA2=1e-5,
        tol=1e-6,
        max_iter=50,
        plot_tag=False,
        runDiffMaps=None,
        seed_num=None,
        tolKDE=0.1,
    ):
        """
        Running the PLoM algorithm to train the model and generate new realizations
        - n_mc: realization/sample size ratio
        - epsilon_pca: tolerance for selecting the number of considered componenets in PCA
        - epsilon_kde: smoothing parameter in the kernel density estimation
        - tol: tolerance in the PLoM iterations
        - max_iter: maximum number of iterations of the PLoM algorithm
        """
        if runDiffMaps == None:
            runDiffMaps = self.runDiffMaps
        else:
            self.runDiffMaps = runDiffMaps

        if plot_tag:
            self.plot_tag = plot_tag
        cur_task = self.task_list.head_task
        while cur_task:
            if cur_task.task_name == 'DataNormalization':
                self.__getattribute__(
                    'task_' + cur_task.task_name
                ).avail_var_list = []
                # data normalization
                self.X_scaled, self.alpha, self.x_min, self.x_mean = (
                    self.DataNormalization(self.X)
                )
                self.logfile.write_msg(
                    msg='PLoM.RunAlgorithm: data normalization completed.',
                    msg_type='RUNNING',
                    msg_level=0,
                )
                self.dbserver.add_item(
                    item_name='X_range', item=self.alpha, data_shape=self.alpha.shape
                )
                self.dbserver.add_item(
                    item_name='X_min',
                    col_names=list(self.X0.columns),
                    item=self.x_min.T,
                    data_shape=self.x_min.shape,
                )
                self.dbserver.add_item(
                    item_name='X_scaled',
                    col_names=list(self.X0.columns),
                    item=self.X_scaled.T,
                    data_shape=self.X_scaled.shape,
                )
                self.dbserver.add_item(
                    item_name='X_scaled_mean',
                    col_names=list(self.X0.columns),
                    item=self.x_mean.T,
                    data_shape=self.x_mean.shape,
                )
                self.logfile.write_msg(
                    msg='PLoM.RunAlgorithm: X_range, X_min, X_scaled and X_scaled_mean saved.',
                    msg_type='RUNNING',
                    msg_level=0,
                )
            elif cur_task.task_name == 'RunPCA':
                self.__getattribute__(
                    'task_' + cur_task.task_name
                ).avail_var_list = []
                # PCA
                self.H, self.mu, self.phi, self.nu, self.errPCA = self.RunPCA(
                    self.X_scaled, epsilon_pca
                )
                self.logfile.write_msg(
                    msg='PLoM.RunAlgorithm: PCA completed.',
                    msg_type='RUNNING',
                    msg_level=0,
                )
                self.dbserver.add_item(
                    item_name='X_PCA',
                    col_names=[
                        'Component' + str(i + 1) for i in range(self.H.shape[0])
                    ],
                    item=self.H.T,
                    data_shape=self.H.shape,
                )
                self.dbserver.add_item(
                    item_name='EigenValue_PCA',
                    item=self.mu,
                    data_shape=self.mu.shape,
                )
                self.dbserver.add_item(
                    item_name='EigenVector_PCA',
                    col_names=['V' + str(i + 1) for i in range(self.phi.shape[1])],
                    item=self.phi,
                    data_shape=self.phi.shape,
                )
                self.dbserver.add_item(
                    item_name='NumComp_PCA', item=np.array([self.nu])
                )
                self.dbserver.add_item(
                    item_name='Error_PCA', item=np.array(self.errPCA)
                )
                self.logfile.write_msg(
                    msg='PLoM.RunAlgorithm: X_PCA, EigenValue_PCA and EigenVector_PCA saved.',
                    msg_type='RUNNING',
                    msg_level=0,
                )
            elif cur_task.task_name == 'RunKDE':
                self.__getattribute__(
                    'task_' + cur_task.task_name
                ).avail_var_list = []
                # parameters KDE
                self.s_v, self.c_v, self.hat_s_v, self.K, self.b = self.RunKDE(
                    self.H, epsilon_kde
                )
                self.logfile.write_msg(
                    msg='PLoM.RunAlgorithm: kernel density estimation completed.',
                    msg_type='RUNNING',
                    msg_level=0,
                )
                self.dbserver.add_item(item_name='s_v', item=np.array([self.s_v]))
                self.dbserver.add_item(item_name='c_v', item=np.array([self.c_v]))
                self.dbserver.add_item(
                    item_name='hat_s_v', item=np.array([self.hat_s_v])
                )
                self.dbserver.add_item(
                    item_name='X_KDE', item=self.K, data_shape=self.K.shape
                )
                self.dbserver.add_item(
                    item_name='EigenValues_KDE', item=self.b, data_shape=self.b.shape
                )
                self.logfile.write_msg(
                    msg='PLoM.RunAlgorithm: KDE, X_KDE and EigenValues_KDE saved.',
                    msg_type='RUNNING',
                    msg_level=0,
                )
                # diff maps
                if runDiffMaps:
                    self.__getattribute__(
                        'task_' + cur_task.task_name
                    ).avail_var_list = []
                    # diff maps
                    self.g, self.m, self.a, self.Z, self.eigenKDE = self.DiffMaps(
                        self.H, self.K, self.b, tol=tolKDE
                    )
                    self.logfile.write_msg(
                        msg='PLoM.RunAlgorithm: diffusion maps completed.',
                        msg_type='RUNNING',
                        msg_level=0,
                    )
                    self.dbserver.add_item(
                        item_name='KDE_g', item=self.g, data_shape=self.g.shape
                    )
                    self.dbserver.add_item(
                        item_name='KDE_m', item=np.array([self.m])
                    )
                    self.dbserver.add_item(
                        item_name='KDE_a', item=self.a, data_shape=self.a.shape
                    )
                    self.dbserver.add_item(
                        item_name='KDE_Z', item=self.Z, data_shape=self.Z.shape
                    )
                    self.dbserver.add_item(
                        item_name='KDE_Eigen',
                        item=self.eigenKDE,
                        data_shape=self.eigenKDE.shape,
                    )
                    self.logfile.write_msg(
                        msg='PLoM.RunAlgorithm: KDE_g, KDE_m, KDE_a, KDE_Z, and KDE_Eigen saved.',
                        msg_type='RUNNING',
                        msg_level=0,
                    )
                else:
                    self.g = np.identity(self.N)
                    self.m = self.N
                    self.a = self.g[:, : self.m].dot(
                        np.linalg.inv(
                            np.transpose(self.g[:, : self.m]).dot(
                                self.g[:, : self.m]
                            )
                        )
                    )
                    self.Z = self.H.dot(self.a)
                    self.eigenKDE = np.array([])
                    self.logfile.write_msg(
                        msg='PLoM.RunAlgorithm: diffusion map is inactivated.',
                        msg_type='RUNNING',
                        msg_level=0,
                    )
                    self.dbserver.add_item(
                        item_name='KDE_g', item=self.g, data_shape=self.g.shape
                    )
                    self.dbserver.add_item(
                        item_name='KDE_m', item=np.array([self.m])
                    )
                    self.dbserver.add_item(
                        item_name='KDE_a', item=self.a, data_shape=self.a.shape
                    )
                    self.dbserver.add_item(
                        item_name='KDE_Z', item=self.Z, data_shape=self.Z.shape
                    )
                    self.dbserver.add_item(
                        item_name='KDE_Eigen',
                        item=self.eigenKDE,
                        data_shape=self.eigenKDE.shape,
                    )
                    self.logfile.write_msg(
                        msg='PLoM.RunAlgorithm: KDE_g, KDE_m, KDE_a, KDE_Z, and KDE_Eigen saved.',
                        msg_type='RUNNING',
                        msg_level=0,
                    )
            elif cur_task.task_name == 'ISDEGeneration':
                self.__getattribute__(
                    'task_' + cur_task.task_name
                ).avail_var_list = []
                # ISDE generation
                self.ISDEGeneration(
                    n_mc=n_mc,
                    tol_PCA2=tol_PCA2,
                    tol=tol,
                    max_iter=max_iter,
                    seed_num=seed_num,
                )
                self.logfile.write_msg(
                    msg='PLoM.RunAlgorithm: Realizations generated.',
                    msg_type='RUNNING',
                    msg_level=0,
                )
                self.dbserver.add_item(
                    item_name='X_new',
                    col_names=list(self.X0.columns),
                    item=self.Xnew.T,
                    data_shape=self.Xnew.shape,
                )
                self.logfile.write_msg(
                    msg='PLoM.RunAlgorithm: X_new saved.',
                    msg_type='RUNNING',
                    msg_level=0,
                )
            else:
                self.logfile.write_msg(
                    msg='PLoM.RunAlgorithm: task {} not found.'.format(
                        cur_task.task_name
                    ),
                    msg_type='ERROR',
                    msg_level=0,
                )
                break
            # refresh status
            for cur_item in TASK_ITEM_MAP[cur_task.task_name]:
                if '/' + cur_item in self.dbserver.get_name_list():
                    self.__getattribute__(
                        'task_' + cur_task.task_name
                    ).avail_var_list.append(cur_item)
            if not cur_task.refresh_status():
                self.logfile.write_msg(
                    msg='PLoM.RunAlgorithm: simulation stopped with task {} not fully completed.'.format(
                        cur_task.task_name
                    ),
                    msg_type='ERROR',
                    msg_level=0,
                )
                break
            # move to the next task
            cur_task = cur_task.next_task

        if self.task_list.refresh_status():
            self.logfile.write_msg(
                msg='PLoM.RunAlgorithm: simulation completed with task(s) {} done.'.format(
                    '->'.join(self.cur_task_list)
                ),
                msg_type='RUNNING',
                msg_level=0,
            )
        else:
            self.logfile.write_msg(
                msg='PLoM.RunAlgorithm: simulation not fully completed.',
                msg_type='ERROR',
                msg_level=0,
            )

    def DataNormalization(self, X):
        """
        Normalizing the X
        - X: the data matrix to be normalized
        """
        # scaling
        X_scaled, alpha, x_min = plom.scaling(X)
        x_mean = plom.mean(X_scaled)

        return X_scaled, alpha, x_min, x_mean

    def RunPCA(self, X_origin, epsilon_pca):
        # ...PCA...
        (H, mu, phi, errors) = plom.PCA(X_origin, epsilon_pca)
        nu = len(H)
        self.logfile.write_msg(
            msg='PLoM.RunPCA: considered number of PCA components = {}'.format(nu),
            msg_type='RUNNING',
            msg_level=0,
        )
        """
        if self.plot_tag:
            fig, ax = plt.subplots(figsize=(8,6))
            ctp = ax.contourf(plom.covariance(H), cmap=plt.cm.bone, levels=100)
            ax.set_xticks(list(range(nu)))
            ax.set_yticks(list(range(nu)))
            ax.set_xticklabels(['PCA-'+str(x+1) for x in range(nu)], fontsize=8, rotation=45)
            ax.set_yticklabels(['PCA-'+str(x+1) for x in range(nu)], fontsize=8, rotation=45)
            ax.set_title('Covariance matrix of PCA')
            cbar = fig.colorbar(ctp)
            plt.savefig(os.path.join(self.vl_path,'PCA_CovarianceMatrix.png'),dpi=480)
            self.logfile.write_msg(msg='PLoM: {} saved in {}.'.format('PCA_CovarianceMatrix.png',self.vl_path),msg_type='RUNNING',msg_level=0)
        """
        return H, mu, phi, nu, errors

    def RunKDE(self, X, epsilon_kde):
        """
        Running Kernel Density Estimation
        - X: the data matrix to be reduced
        - epsilon_kde: smoothing parameter in the kernel density estimation
        """
        (s_v, c_v, hat_s_v) = plom.parameters_kde(X)
        K, b = plom.K(X, epsilon_kde)

        return s_v, c_v, hat_s_v, K, b

    def DiffMaps(self, H, K, b, tol=0.1):
        # ..diff maps basis...
        # self.Z = PCA(self.H)
        try:
            g, eigenvalues = plom.g(K, b)  # diffusion maps
            g = g.real
            m = plom.m(eigenvalues, tol=tol)
            a = g[:, 0:m].dot(np.linalg.inv(np.transpose(g[:, 0:m]).dot(g[:, 0:m])))
            Z = H.dot(a)
            """
            if self.plot_tag:
                fig, ax = plt.subplots(figsize=(6,4))
                ax.semilogy(np.arange(len(eigenvalues)), eigenvalues)
                ax.set_xlabel('Eigen number')
                ax.set_ylabel('Eigen value')
                ax.set_title('Eigen value (KDE)')
                plt.savefig(os.path.join(self.vl_path,'KDE_EigenValue.png'),dpi=480)
                self.logfile.write_msg(msg='PLoM: {} saved in {}.'.format('KDE_EigenValue.png',self.vl_path),msg_type='RUNNING',msg_level=0)
            """
        except:
            g = None
            m = 0
            a = None
            Z = None
            eigenvalues = []
            self.logfile.write_msg(
                msg='PLoM.DiffMaps: diffusion maps failed.',
                msg_type='ERROR',
                msg_level=0,
            )

        return g, m, a, Z, eigenvalues

    def ISDEGeneration(
        self, n_mc=5, tol_PCA2=1e-5, tol=0.02, max_iter=50, seed_num=None
    ):
        """
        The construction of a nonlinear Ito Stochastic Differential Equation (ISDE) to generate realizations of random variable H
        """
        if seed_num:
            np.random.seed(seed_num)
        # constraints
        if self.g_c:
            self.C_h_hat_eta = plom.covariance(
                self.g_c(self.x_mean + (self.phi).dot(np.diag(self.mu)).dot(self.H))
            )

            # scaling beta
            # self.beta_c_normalized = self.beta_c_aux(self.beta_c, self.x_min, self.alpha)
            # KZ, 07/24
            self.beta_c_normalized = self.beta_c_aux(self.beta_c, self.X)
            self.b_c, self.psi = plom.PCA2(
                self.C_h_hat_eta, self.beta_c_normalized, tol_PCA2
            )
            self.nu_c = len(self.b_c)

            self.hessian = plom.hessian_gamma(
                self.H, self.psi, self.g_c, self.phi, self.mu, self.x_mean
            )
            self.inverse = plom.solve_inverse(self.hessian)

            self.gradient = plom.gradient_gamma(
                self.b_c, self.H, self.g_c, self.phi, self.mu, self.psi, self.x_mean
            )

            self.lambda_i = -(self.inverse).dot(self.gradient)

            self.errors = [plom.err(self.gradient, self.b_c)]
            iteration = 0
            nu_init = np.random.normal(size=(int(self.nu), int(self.N)))
            self.Y = nu_init.dot(self.a)

            error_ratio = 0
            increasing_iterations = 0

            while (
                iteration < max_iter
                and self.errors[iteration] > tol * self.errors[0]
                and (increasing_iterations < 3)
            ):
                self.logfile.write_msg(
                    msg='PLoM.ISDEGeneration: running iteration {}.'.format(
                        iteration + 1
                    ),
                    msg_type='RUNNING',
                    msg_level=0,
                )
                Hnewvalues, nu_lambda, x_, x_2 = plom.generator(
                    self.Z,
                    self.Y,
                    self.a,
                    n_mc,
                    self.x_mean,
                    self.H,
                    self.s_v,
                    self.hat_s_v,
                    self.mu,
                    self.phi,
                    self.g[:, 0 : int(self.m)],
                    psi=self.psi,
                    lambda_i=self.lambda_i,
                    g_c=self.g_c,
                    D_x_g_c=self.D_x_g_c,
                )  # solve the ISDE in n_mc iterations

                self.gradient = plom.gradient_gamma(
                    self.b_c,
                    Hnewvalues,
                    self.g_c,
                    self.phi,
                    self.mu,
                    self.psi,
                    self.x_mean,
                )
                self.hessian = plom.hessian_gamma(
                    Hnewvalues, self.psi, self.g_c, self.phi, self.mu, self.x_mean
                )
                self.inverse = plom.solve_inverse(self.hessian)

                self.lambda_i = self.lambda_i - 0.3 * (self.inverse).dot(
                    self.gradient
                )

                self.Z = Hnewvalues[:, -self.N :].dot(self.a)
                self.Y = nu_lambda[:, -self.N :].dot(self.a)
                iteration += 1

                (self.errors).append(plom.err(self.gradient, self.b_c))

                if error_ratio > 1.00:
                    increasing_iterations += 1
                else:
                    increasing_iterations = 0

            # saving data
            self.dbserver.add_item(
                item_name='Errors',
                item=np.array(self.errors),
                data_shape=np.array(self.errors).shape,
            )

            if iteration == max_iter:
                self.logfile.write_msg(
                    msg='PLoM.ISDEGeneration: max. iteration reached and convergence not achieved.',
                    msg_type='WARNING',
                    msg_level=0,
                )

        # no constraints
        else:
            nu_init = np.random.normal(size=(int(self.nu), int(self.N)))
            self.Y = nu_init.dot(self.a)
            Hnewvalues, nu_lambda, x_, x_2 = plom.generator(
                self.Z,
                self.Y,
                self.a,
                n_mc,
                self.x_mean,
                self.H,
                self.s_v,
                self.hat_s_v,
                self.mu,
                self.phi,
                self.g[:, 0 : int(self.m)],
                seed_num=seed_num,
            )  # solve the ISDE in n_mc iterations
            self.logfile.write_msg(
                msg='PLoM.ISDEGeneration: new generations are simulated.',
                msg_type='RUNNING',
                msg_level=0,
            )
            self.dbserver.add_item(item_name='Errors', item=np.array([0]))

            # saving data
            self.errors = []
            self.dbserver.add_item(
                item_name='Errors',
                item=np.array(self.errors),
                data_shape=np.array(self.errors).shape,
            )

        self.Xnew = self.x_mean + self.phi.dot(np.diag(self.mu)).dot(Hnewvalues)

        # unscale
        self.Xnew = np.diag(self.alpha).dot(self.Xnew) + self.x_min

    def export_results(self, data_list=[], file_format_list=['csv']):
        """
        Exporting results by the data names
        - data_list: list of data names
        - file_format_list: list of output formats
        """
        avail_name_list = self.dbserver.get_name_list()
        if not data_list:
            # print available data names
            avail_name_str = ','.join(avail_name_list)
            self.logfile.write_msg(
                msg='PLoM.export_results: available data {}.'.format(avail_name_str),
                msg_type='RUNNING',
                msg_level=0,
            )
        if not avail_name_list:
            # empty database
            self.logfile.write_msg(
                msg='PLoM.export_results: database is empty - no data exported.',
                msg_type='ERROR',
                msg_level=0,
            )
        else:
            for tag, data_i in enumerate(data_list):
                if data_i not in avail_name_list:
                    self.logfile.write_msg(
                        msg='PLoM.export_results: {} is not found and skipped.'.format(
                            data_i
                        ),
                        msg_type='WARNING',
                        msg_level=0,
                    )
                else:
                    try:
                        ff_i = file_format_list[tag]
                    except:
                        ff_i = file_format_list[-1]
                    ex_flag = self.dbserver.export(
                        data_name=data_i, file_format=ff_i
                    )
                    if type(ex_flag) == int and ex_flat == 1:
                        self.logfile.write_msg(
                            msg='PLoM.export_results: {} is not found and skipped.'.format(
                                data_i
                            ),
                            msg_type='WARNING',
                            msg_level=0,
                        )
                    elif type(ex_flag) == int and ex_flag == 2:
                        self.logfile.write_msg(
                            msg='PLoM.export_results: {} is not supported yest.'.format(
                                ff_i
                            ),
                            msg_type='ERROR',
                            msg_level=0,
                        )
                    else:
                        self.logfile.write_msg(
                            msg='PLoM.export_results: {} is exported in {}.'.format(
                                data_i, ex_flag
                            ),
                            msg_type='RUNNING',
                            msg_level=0,
                        )

    """
    def PostProcess():
    	#...output plots...

    	#plot some histograms
        import matplotlib.patches as mpatches
        plt.plot(Xnewvalues[0,:], Xnewvalues[1,:], 'rx')
        plt.plot(Xvalues[0], Xvalues[1], 'bo')
        plt.xlabel('x')
        plt.ylabel('y')
        red_patch = mpatches.Patch(color='red', label='X_c')
        blue_patch = mpatches.Patch(color='blue', label='X')
        plt.legend(handles=[red_patch, blue_patch])
        plt.show()

        import matplotlib.patches as mpatches
        plt.xlabel('x1')
        plt.subplot(2,1,1)
        plt.hist(Xnewvalues[0], bins = 100, color = 'red')
        plt.subplot(2,1,2)
        plt.hist(Xvalues[0], bins = 100, color = 'blue')
        plt.title('Histogram')
        red_patch = mpatches.Patch(color='red', label='X_c')
        blue_patch = mpatches.Patch(color='blue', label='X')
        plt.legend(handles=[red_patch, blue_patch])
        plt.shlow()

        import matplotlib.patches as mpatches
        plt.xlabel('x2')
        plt.subplot(2,1,1)
        plt.hist(Xnewvalues[1], bins = 100, color = 'red')
        plt.subplot(2,1,2)
        plt.hist(Xvalues[1], bins = 100, color = 'blue')
        plt.title('Histogram')
        red_patch = mpatches.Patch(color='red', label='X new realizations')
        blue_patch = mpatches.Patch(color='blue', label='X')
        plt.legend(handles=[red_patch, blue_patch])
        plt.show()

        import matplotlib.patches as mpatches
        plt.xlabel('x3')
        plt.subplot(2,1,1)
        plt.hist(Xnewvalues[2], bins = 100, color = 'red')
        plt.subplot(2,1,2)
        plt.hist(Xvalues[2], bins = 100, color = 'blue')
        plt.title('Histogram')
        red_patch = mpatches.Patch(color='red', label='X new realizations')
        blue_patch = mpatches.Patch(color='blue', label='X')
        plt.legend(handles=[red_patch, blue_patch])
        plt.show()

        import matplotlib.patches as mpatches
        plt.xlabel('x4')
        plt.subplot(2,1,1)
        plt.hist(Xnewvalues[3], bins = 100, color = 'red')
        plt.subplot(2,1,2)
        plt.hist(Xvalues[3], bins = 100, color = 'blue')
        plt.title('Histogram')
        red_patch = mpatches.Patch(color='red', label='X new realizations')
        blue_patch = mpatches.Patch(color='blue', label='X')
        plt.legend(handles=[red_patch, blue_patch])
        plt.show()
    """
